{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\Logo_UCLL_ENG_RGB.png\" style=\"background-color:white;\" />\n",
    "\n",
    "# Data Analytics & Machine learning\n",
    "\n",
    "Lecturers: Aimée Lynn Backiel, Kenric Borgelioen, Sofie Torfs, Lies Bollens\n",
    "\n",
    "Academic year 2025-2026\n",
    "\n",
    "## Lab 6: Machine learning, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture outline\n",
    "\n",
    "1. Recap of previous weeks\n",
    "2. Overfitting and underfitting\n",
    "3. Automating machine learning pipelines with sci-kit learn\n",
    "4. Model evaluation \n",
    "5. parameters and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of last lecture(s)\n",
    "\n",
    "#### Lab 1\n",
    "\n",
    "1. We ensured we had a valid Python installation.\n",
    "2. We learnt what a virtual environment is:\n",
    "   * Isolated Python executable and packages.\n",
    "   * We created a virtual environment.\n",
    "3. Absolute path vs relative path recap.\n",
    "4. Recap of data structures in Python\n",
    "\n",
    "#### Lab 2\n",
    "1. Installed Pandas\n",
    "2. Learnt how to read data\n",
    "3. Learnt how to calculate mean, mode, median etc.\n",
    "4. Basic exploration of the 4 variables\n",
    "\n",
    "#### Lab 3\n",
    "1. Wrapped up computing summary statistics (mean, median, mode, ...)\n",
    "2. Learnt how to deal with outliers \n",
    "3. Focused on exploration of data\n",
    "\n",
    "#### Lab 4\n",
    "1. Univariate data visualization using Matplotlib\n",
    "   1. Figures and axes\n",
    "   2. Histograms\n",
    "   3. Box plots\n",
    "   4. Bar charts\n",
    "2. Multivariate data visualization using Seaborn\n",
    "   1. Scatter plots\n",
    "   2. Small multiples\n",
    "   3. Color coding\n",
    "\n",
    "#### Lab 5\n",
    "2. Intro to machine learning using scikit-learn\n",
    "   1. Preprocessing\n",
    "      1. One Hot encoding\n",
    "      2. Scaling\n",
    "      3. Outliers\n",
    "   2. Classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada Turing Travelogue, or as everyone calls her, Ada just started working part time at her parents travel agency. She has a keen understanding and interest of everything related to applied computer science ranging from server & system management to full stack software development. Through database foundations she already understands how to query data and programming 1 and 2 covered the essentials about the Python programming language. Recently she has just decided to start learning about data analytics & machine learning as well.\n",
    "\n",
    "She uses her skills to connect to the travel agency's database where she finds many, normalized, tables. Ada recalls what she learnt in database foundations and performs all the correct joins. Afterwards she saves the data in the `data/` folder.\n",
    "\n",
    "\n",
    "She finds the following dataset:\n",
    "\n",
    "| Column Name          | Description                                                                                       |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| SalesID              | Unique identifier for each sale.                                                                  |\n",
    "| Age                  | Age of the traveler.                                                                              |\n",
    "| Country              | Country of origin of the traveler.                                                                |\n",
    "| Membership_Status    | Membership level of the traveler in the booking system; could be 'standard', 'silver', or 'gold'. |\n",
    "| Previous_Purchases   | Number of previous bookings made by the traveler.                                                 |\n",
    "| Destination          | Travel destination chosen by the traveler.                                                        |\n",
    "| Stay_length          | Duration of stay at the destination.                                                              |\n",
    "| Guests               | Number of guests traveling (including the primary traveler).                                             |\n",
    "| Travel_month         | Month in which the travel is scheduled.                                                           |\n",
    "| Months_before_travel | Number of months prior to travel that the booking was made.                                       |\n",
    "| Earlybird_discount   | Boolean flag indicating whether the traveler received an early bird discount.                     |\n",
    "| Package_Type         | Type of travel package chosen by the traveler.                                                    |\n",
    "| Cost                 | Calculated cost of the travel package.                                                            |\n",
    "| Margin | The cost (for the traveler) - what the travel agency pays. |\n",
    " | Additional_Services_Cost| The amount of additional services (towels, car rentals, room service, ...) that was bought during the trip. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our challenge\n",
    "\n",
    "Before getting into harder use cases we will start off by predicting the cost of a given stay. Right now Ada's parents do this manually automating this task would already be a big help to their business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.datascience-pm.com/wp-content/uploads/2021/02/CRISP-DM.png\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also helps to situate our progress within CRISP-DM. We have done the first three steps, as from this lecture we will progress to modeling. As mentioned in the lecture, this is an iterative procedure, as we are doing modeling we need to circle back to both data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning with sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What have we done so far of the image below? What stages have we completed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done train test splitting and we have built a few models on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is when the model doesn't learn general patterns from the data but rather focuses on doing really well on the training data. \n",
    "\n",
    "❗ Typically it means that the performance on the test set will be a lot worse than that on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting is when the model learns a pattern that doesn't significantly capture the details of the training set.\n",
    "\n",
    "❗ Typically it means that the performance on the test set will be similarly (bad) on as that on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://www.mathworks.com/discovery/overfitting/_jcr_content/mainParsys/image.adapt.full.medium.svg/1686825007300.svg\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out continuing where we left of last session.\n",
    "Create two different regression models that predict cost based on the other variables in the dataset.\n",
    "Use a linear regression model and a decision tree regression.\n",
    "Compare both methods in terms of performance.\n",
    "We'll get you some code and tips to get you started, the rest is up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # by convention\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import numpy  as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data\n",
    "#X = #features are denoted with a capital X\n",
    "#y = #the target variable is denoted with a small y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_dataset = pd.read_csv(\"data/lab_6_dataset.csv\")\n",
    "X = travel_dataset.drop(columns=\"cost\") #features are denoted with a capital X\n",
    "y = travel_dataset[\"cost\"] #the target variable is denoted with a small y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Should we apply our preprocessing step to the training dataset, the test dataset, or both?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proper methodology is to fit the OneHotEncoder and StandardScaler on the training data only. This way, they \"learn\" the categories of the categorical variables and the distribution (mean and standard deviation) of the numerical variables from the training set. Once fitted, these preprocessors should then be applied to the test data, ensuring that the transformation applied is consistent and does not give the model any information about the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\"package_Type\", \"destination\", \"country\"]\n",
    "numeric_columns = [\"guests\", \"age\", \"stay_length\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_preprocessors(train_data: np.ndarray, categorical_columns: list[str], numeric_columns: list[str]) -> (OneHotEncoder, StandardScaler):\n",
    "    # Fit the OneHotEncoder and StandardScaler to the training data\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    scaler = StandardScaler()\n",
    "    ohe.fit(train_data[categorical_columns])\n",
    "    scaler.fit(train_data[numeric_columns])\n",
    "    \n",
    "    # Return the fitted preprocessors\n",
    "    return ohe, scaler\n",
    "\n",
    "def transform_data(data: np.ndarray, categorical_columns: list[str], numeric_columns: list[str], ohe: OneHotEncoder, scaler: StandardScaler) -> np.ndarray:\n",
    "    # Transform data using the already fitted preprocessors\n",
    "    cat_cols = ohe.transform(data[categorical_columns])\n",
    "    num_cols = scaler.transform(data[numeric_columns])\n",
    "    \n",
    "    # Return the transformed data\n",
    "    return np.hstack((cat_cols, num_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe, scaler = fit_preprocessors(X_train, cat_columns, numeric_columns)\n",
    "X_train_preprocessed = transform_data(X_train, cat_columns, numeric_columns, ohe=ohe, scaler=scaler)\n",
    "X_test_preprocessed = transform_data(X_test, cat_columns, numeric_columns, ohe=ohe, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a linear regression model and predict both the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_preprocessed, y_train)\n",
    "predictions_lin_reg = lin_reg.predict(X_train_preprocessed)\n",
    "predictions_test_lin_reg= lin_reg.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Plot both the predictions of the training and the test set compared to the actual labels. \n",
    "You'll see that you basically have to create the same plot multiple times.\n",
    "Tip: create a function that can help you avoid repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(y_true: pd.Series, predictions: np.ndarray, title: str) -> None:\n",
    "    \n",
    "    fig = px.scatter(x=predictions, y=y_true, labels={\"x\": \"predicted\", \"y\": \"actual\"}, title=title)\n",
    "    fig.add_shape(type=\"line\",\n",
    "                x0=-1000, \n",
    "                y0=-1000, \n",
    "                x1=7000, \n",
    "                y1=7000)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to plot the comparison on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results( y_train, predictions_lin_reg, title=\"predicted vs actual for linear regression on the training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to plot the comparison on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_test, predictions_test_lin_reg, title=\"predicted versus actual on the test set for linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Repeat the process for the decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit(X_train_preprocessed, y_train)\n",
    "predictions_dt = decision_tree.predict(X_train_preprocessed)\n",
    "predictions_test_dt = decision_tree.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_train, predictions_dt, title=\"predicted versus actual on the training set for a decision tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_test, predictions_test_dt, title=\"predicted versus actual on the test set for a decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Which of the two models is overfitting? Can you describe how and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree is overfitting. It's more or less analogous to the first image, the error is exactly 0 for many values in the training set, that's already a danger sign. Upon closer inspection of the test we can see that the error there there is far longer. We can say that the model has failed to learn a general pattern. The pattern it learnt describes only the training set well and nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Which of the two models is underfitting? Can you describe how and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression is underfitting. The model is too simplistic, it is unable to to account for certain important patterns and combinations of variables. The error it makes is large both on the training and test set. We can do better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pipelines to avoid mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the preprocessing step is critical to prepare the data for modeling. However, there's a subtle but crucial methodological error in the function shown. The issue lies in the lines where `OneHotEncoder` and `StandardScaler` are fitted:\n",
    "\n",
    "```python\n",
    "def preprocess_data(data: np.ndarray, categorical_columns: list[str], numeric_columns: list[str]) -> np.ndarray:\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False) \n",
    "    ohe.fit(data[cat_columns]) # The error occurs here!\n",
    "    cat_cols_train  = ohe.transform(data[categorical_columns])\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols_train = scaler.fit_transform(data[numeric_columns]) # And here!\n",
    "\n",
    "    return np.hstack((cat_cols_train, num_cols_train))  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key mistake in the preprocessing function provided lies in fitting the OneHotEncoder and StandardScaler to the data inside the function. This could lead to a situation where, when preprocessing the test set, the encoders and scalers are fitted again separately with the test set statistics, which is incorrect. The preprocessing steps that \"learn\" from the data, such as encoding categorical variables and scaling numerical variables, should be based solely on the training data to avoid introducing bias from the test set into the model.\n",
    "\n",
    "The proper methodology is to fit the OneHotEncoder and StandardScaler on the training data only. This way, they \"learn\" the categories of the categorical variables and the distribution (mean and standard deviation) of the numerical variables from the training set. Once fitted, these preprocessors should then be applied to the test data, ensuring that the transformation applied is consistent and does not give the model any information about the test set.\n",
    "\n",
    "One potential way to solve it is as follows:\n",
    "\n",
    "```python\n",
    "def fit_preprocessors(train_data: np.ndarray, categorical_columns: list[str], numeric_columns: list[str]) -> (OneHotEncoder, StandardScaler):\n",
    "    # Fit the OneHotEncoder and StandardScaler to the training data\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    scaler = StandardScaler()\n",
    "    ohe.fit(train_data[categorical_columns])\n",
    "    scaler.fit(train_data[numeric_columns])\n",
    "    \n",
    "    # Return the fitted preprocessors\n",
    "    return ohe, scaler\n",
    "\n",
    "def transform_data(data: np.ndarray, categorical_columns: list[str], numeric_columns: list[str], ohe: OneHotEncoder, scaler: StandardScaler) -> np.ndarray:\n",
    "    # Transform data using the already fitted preprocessors\n",
    "    cat_cols = ohe.transform(data[categorical_columns])\n",
    "    num_cols = scaler.transform(data[numeric_columns])\n",
    "    \n",
    "    # Return the transformed data\n",
    "    return np.hstack((cat_cols, num_cols))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, the Pipeline and ColumnTransformer classes offer an idiomatic and streamlined way to chain multiple preprocessing steps and a model into a single workflow. The `Pipeline` class allows you to assemble sequences of transformations and a final model, which simplifies your code and helps prevent common mistakes, such as fitting preprocessing steps to the test data. The `ColumnTransformer` is particularly useful for applying different preprocessing to different columns, such as one-hot encoding for categorical variables and scaling for numerical variables. By combining these tools, you not only reduce the need for manual 'glue' code but also safeguard against the leakage of information from the test set into the training process. We highly recommend you always use this in the scope of this course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax is quite simple:\n",
    "\n",
    "*  `make_column_transformer` expects multiple tuples. The `transformer` (preprocessing step) is in the first position of the tuple and the columns you apply the transformer (type: `list[str]`) is in the second position.  \n",
    "*  `make_pipeline` similarly expects all objects in sequence, so typically you add your preprocessing first followed by the model you want to apply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = make_column_transformer(\n",
    "    (StandardScaler(), numeric_columns),\n",
    "    (OneHotEncoder(sparse_output=False), cat_columns),\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_pipe = make_pipeline(preprocessing, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_pipe.fit(X_train, y_train)\n",
    "predictions_lin_reg_train = lin_reg_pipe.predict(X_train)\n",
    "predictions_lin_reg_test = lin_reg_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all!\n",
    "\n",
    "Convince yourself for a second that is a lot simpler than what we were doing previously, we simply use `make_column_transformer` to indicate what preprocessing we want to apply to which column. Afterwards we put our preprocessing in a pipeline with the model we want to use.\n",
    "\n",
    "❗ Once you call `model.fit(X_train, y_train)` it both fits the preprocessing and the model in one go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines make it easy to make many different models in one go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Create a function that can train 4 models in one go: Linear Regression, Decision Tree, Random Forest and HistGradient Boosting.\n",
    "Use the components you already created above and the 'make_pipeline' function.\n",
    "Store your results in a list called 'results'. Create a tuple for eacht of the techniques, including the name of the technique, the predictions on the training set and the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_pipe = make_pipeline(preprocessing, DecisionTreeRegressor())\n",
    "rf_pipe = make_pipeline(preprocessing, RandomForestRegressor())\n",
    "xgb_pipe = make_pipeline(preprocessing, HistGradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even do the following if you want:\n",
    "\n",
    "```python\n",
    "\n",
    "model_name_pair = [(\"random_forest\", RandomForestRegressor()), (\"gradient boosting\", HistGradientBoostingRegressor()), (\"decision tree\", DecisionTreeRegressor())]\n",
    "results = []\n",
    "for pair in model_name_pair:\n",
    "    name, model = pair\n",
    "    pipe = make_pipeline(preprocessing, model)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predictions_train = pipe.predict(X_train)\n",
    "    predictions_test = pipe.predict(X_test)\n",
    "    result.append([(name, predictions_train, predictions_test)])\n",
    "```\n",
    "\n",
    "The code above would train 3 models in a single for loop. Afterwards you can use a single function to evaluate the results you have obtained from all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we've only *qualitatively* judged the quality of our models by looking at our predicted versus actual plot. We're interested in having a single number that summarizes the the performance of our model. The reason is that investigating graphs doesn't scale well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt 0: taking the mean of the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first intuition might be to take the difference of the predictions and the actual values. This is called the **error** or the **residual**. After we have this value we may be tempted to take the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = predictions_lin_reg_test - y_test\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(error, title=\"distribution of the errors of linear regression on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Why is the mean of residuals misleading?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the mean the negative errors and the positive errors cancel each other out. This is definitely not what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ How can we better quantify errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by taking the absolute value of the errors and then taking the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt 1: taking the mean of the absolute error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(np.abs(error), title=\"distribution of the absolute errors of linear regression on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is more informative as it provides a clearer picture of how much error is present on average in our predictions.\n",
    "\n",
    "By considering the MAE, we obtain a useful summary statistic for model performance that is easy to understand and directly interpretable in terms of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice squaring the errors is more common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt 2: Taking the mean of the squared errors (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squaring the errors, as shown in the equation: $(-2)² = 4 = (2)²$, ensures that all error values are positive. This method has two main benefits:\n",
    "\n",
    "This method has two main advantages:\n",
    "\n",
    "1. Large errors are amplified more than smaller ones, which can be particularly important in cases where larger deviations are less tolerable.\n",
    "2. MSE is a differentiable function, which makes it mathematically convenient for optimization algorithms used in model training. There are other statistical properties that the MSE can leverage.\n",
    "\n",
    "Note: point 2 is only for your information. You don't need to know this at all. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.square(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since MSE can result in large numbers that are difficult to interpret, we often take the square root to obtain the Root Mean Squared Error (RMSE), which has the same units as the original values:\n",
    "\n",
    "*Note: the root is $\\sqrt(x)$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(np.square(error), title=\"distribution of the square errors of linear regression on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What are downsides of the MSE? There are two, but one is harder to come up with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Easier: The MSE can be more difficult to interpret since it's not in the same units as the original data. Taking the square root to obtain the RMSE helps mitigate this issue.\n",
    "* Harder: MSE is sensitive to outliers because errors are squared, so outliers have a disproportionately large impact on the total error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "Our advice:\n",
    "\n",
    "* Compute both the MAE and the RMSE. \n",
    "* The one you should focus on the most is typically the RMSE. \n",
    "* If there are error outliers then the difference between the MAE and the RMSE is likely going to be large. In that case it is typically more interesting to look at the MAE. \n",
    " \n",
    "Typically, RMSE is preferred because it is more sensitive to large errors, which can be critical in applications where such errors are especially problematic, like in autonomous vehicle guidance systems. However, if your model is prone to outliers, or if large errors are less impactful, MAE can be a more relevant metric. Always consider the specific context of your application when choosing your primary evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model evaluation using sci-kit learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sci-kit learn offers all of these functions out of the box. All you need to do is remember their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Use the pipeline approach discussed above and the mean_absolute_error and mean_squared_error for the following models: RandomForestRegressor, HistGradientBoostingRegressor, DecisionTreeRegressor and LinearRegression. You can build on the function that you created above that returned results for each of the regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_pair = [(\"random_forest\", RandomForestRegressor(n_jobs=-1)), (\"gradient boosting\", HistGradientBoostingRegressor()), (\"decision tree\", DecisionTreeRegressor()), (\"linear regression\", LinearRegression()) ]\n",
    "results = []\n",
    "for pair in model_name_pair:\n",
    "    name, model = pair\n",
    "    print(f\"STARTING {name}\")\n",
    "    pipe = make_pipeline(preprocessing, model)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predictions_train = pipe.predict(X_train)\n",
    "    predictions_test = pipe.predict(X_test)\n",
    "    print(\"-\"*20)\n",
    "    print(f\"The MAE on the training set is {mean_absolute_error(y_train, predictions_train)} and on the test set it is {mean_absolute_error(y_test, predictions_test)}\")\n",
    "    print(f\"The mse on the training set is {mean_squared_error(y_train, predictions_train, squared=False)} and on the test set it is {mean_squared_error(y_test, predictions_test, squared=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Comment on the behavior of the models. Are they overfitting? Underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the decision tree and random forest are both overfitting. There is a large gap between the performance on the test set and the training set.\n",
    "Linear regression is underfitting, the performance on test and train is similarly bad. \n",
    "Gradient boosting is not underfitting nor overfitting. The difference between test and train is minimal, on top of that, its MAE and MSE are better than all the alternatives we have tried."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
