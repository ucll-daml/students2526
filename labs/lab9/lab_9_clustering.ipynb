{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\Logo_UCLL_ENG_RGB.png\" style=\"background-color:white;\" />\n",
    "\n",
    "# Data Analytics & Machine learning\n",
    "\n",
    "Lecturers: Aimée Lynn Backiel, Kenric Borgelioen, Sofie Torfs, Lies Bollens\n",
    "\n",
    "Academic year 2025-2026\n",
    "\n",
    "## Lab 9: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture outline\n",
    "\n",
    "1. Unsupervised learning: clustering\n",
    "    * Partitional clustering\n",
    "    * Hierarchcal clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of last lecture(s)\n",
    "\n",
    "#### Lab 1\n",
    "\n",
    "1. We ensured we had a valid Python installation.\n",
    "2. We learnt what a virtual environment is:\n",
    "   * Isolated Python executable and packages.\n",
    "   * We created a virtual environment.\n",
    "3. Absolute path vs relative path recap.\n",
    "4. Recap of data structures in Python\n",
    "\n",
    "#### Lab 2\n",
    "1. Installed Pandas\n",
    "2. Learnt how to read data\n",
    "3. Learnt how to calculate mean, mode, median etc.\n",
    "4. Basic exploration of the 4 variables\n",
    "\n",
    "#### Lab 3\n",
    "1. Wrapped up computing summary statistics (mean, median, mode, ...)\n",
    "2. Learnt how to deal with outliers \n",
    "3. Focused on exploration of data\n",
    "\n",
    "#### Lab 4\n",
    "1. Univariate data visualization using Matplotlib\n",
    "   1. Figures and axes\n",
    "   2. Histograms\n",
    "   3. Box plots\n",
    "   4. Bar charts\n",
    "2. Multivariate data visualization using Seaborn\n",
    "   1. Scatter plots\n",
    "   2. Small multiples\n",
    "   3. Color coding\n",
    "\n",
    "#### Lab 5\n",
    "1. Intro to machine learning using scikit-learn\n",
    "   1. Preprocessing\n",
    "      1. One Hot encoding\n",
    "      2. Scaling\n",
    "      3. Outliers\n",
    "   2. Regression\n",
    "\n",
    "#### Lab 6\n",
    "1. Preprocessing with scikit-learn\n",
    "   1. ColumnTransformer: Apply a transformation to specific columns.\n",
    "   2. Pipeline: Do several transformations after each other\n",
    "2. Evaluation:\n",
    "   1. Why the mean of the error is a bad idea\n",
    "   2. Mean absolute error\n",
    "   3. Mean squared error\n",
    "\n",
    "#### Lab 7\n",
    "1. Feature engineering\n",
    "   1. Binning\n",
    "   2. Interactions\n",
    "   3. Custom features\n",
    "2. Rounding up model evaluation\n",
    "   1. Cross validation\n",
    "   2. Hyper parameter tuning\n",
    "\n",
    "#### Lab 8\n",
    "1. One large exercise covering all we have done so far in this course applied to classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our latest case: Hotel booking dataset\n",
    "\n",
    "source: https://www.sciencedirect.com/science/article/pii/S2352340918315191\n",
    "\n",
    "*This data article describes two datasets with hotel demand data. One of the hotels (H1) is a resort hotel and the other is a city hotel (H2). Both datasets share the same structure, with 31 variables describing the 40,060 observations of H1 and 79,330 observations of H2. Each observation represents a hotel booking. Both datasets comprehend bookings due to arrive between the 1st of July of 2015 and the 31st of August 2017, including bookings that effectively arrived and bookings that were canceled. Both hotels are located in Portugal: H1 at the resort region of Algarve and H2 at the city of Lisbon.*\n",
    "\n",
    "The goal is to help the two hotels maximize their revenue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|variable                       |class     |description |\n",
    "|:------------------------------|:---------|:-----------|\n",
    "|hotel                          |character | 'Resort hotel' or 'City hotel' |\n",
    "|is_canceled                    |double    | Value indicating if the booking was canceled (1) or not (0) |\n",
    "|lead_time                      |double    | Number of days that elapsed between the entering date of the booking into the PMS and the arrival date |\n",
    "|arrival_date_year              |double    | Year of arrival date|\n",
    "|arrival_date_month             |character | Month of arrival date|\n",
    "|arrival_date_week_number       |double    | Week number of year for arrival date|\n",
    "|arrival_date_day_of_month      |double    | Day of arrival date|\n",
    "|stays_in_weekend_nights        |double    | Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel |\n",
    "|stays_in_week_nights           |double    |  Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel|\n",
    "|adults                         |double    | Number of adults|\n",
    "|children                       |double    | Number of children|\n",
    "|babies                         |double    |Number of babies |\n",
    "|meal                           |character | Type of meal booked. Categories are presented in standard hospitality meal packages: <br> Undefined/SC – no meal package;<br>BB – Bed & Breakfast; <br> HB – Half board (breakfast and one other meal – usually dinner); <br> FB – Full board (breakfast, lunch and dinner) |\n",
    "|country                        |character | Country of origin. Categories are represented in the ISO 3155–3:2013 format |\n",
    "|market_segment                 |character | Market segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators” |\n",
    "|distribution_channel           |character | Booking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators” |\n",
    "|is_repeated_guest              |double    | Value indicating if the booking name was from a repeated guest (1) or not (0) |\n",
    "|previous_cancellations         |double    | Number of previous bookings that were cancelled by the customer prior to the current booking |\n",
    "|previous_bookings_not_canceled |double    | Number of previous bookings not cancelled by the customer prior to the current booking |\n",
    "|reserved_room_type             |character | Code of room type reserved. Code is presented instead of designation for anonymity reasons |\n",
    "|assigned_room_type             |character | Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons |\n",
    "|booking_changes                |double    | Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation|\n",
    "|deposit_type                   |character | Indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:<br>No Deposit – no deposit was made;<br>Non Refund – a deposit was made in the value of the total stay cost;<br>Refundable – a deposit was made with a value under the total cost of stay. |\n",
    "|agent                          |character | ID of the travel agency that made the booking |\n",
    "|company                        |character | ID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons |\n",
    "|days_in_waiting_list           |double    | Number of days the booking was in the waiting list before it was confirmed to the customer |\n",
    "|customer_type                  |character | Type of booking, assuming one of four categories:<br>Contract - when the booking has an allotment or other type of contract associated to it;<br>Group – when the booking is associated to a group;<br>Transient – when the booking is not part of a group or contract, and is not associated to other transient booking;<br>Transient-party – when the booking is transient, but is associated to at least other transient booking|\n",
    "|adr                            |double    | Average Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights |\n",
    "|required_car_parking_spaces    |double    | Number of car parking spaces required by the customer |\n",
    "|total_of_special_requests      |double    | Number of special requests made by the customer (e.g. twin bed or high floor)|\n",
    "|reservation_status             |character | Reservation last status, assuming one of three categories:<br>Canceled – booking was canceled by the customer;<br>Check-Out – customer has checked in but already departed;<br>No-Show – customer did not check-in and did inform the hotel of the reason why |\n",
    "|reservation_status_date        |double    | Date at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hotel_df = pd.read_csv(\"data/lab9.csv\")\n",
    "hotel_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ In the previous lab, we focused on finding out who cancels a reservation. This was a classification problem. This time, we'll try to find groups in the customers, which is a clustering problem. Can you explain why these two are different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to lab 6 and take another look on how to prepare your data for modeling. We'll also need to use one-hot encoding on our categorical data and scaling on our numerical data. By now, you should know how to do this yourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT THE RIGHT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHOOSE YOUR NUMERICAL AND CATEGORICAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD A PREPROCESSOR NAMED 'preprocessor'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitional Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will start with partitional clustering. Go back to the theory slides and look up what this means.\n",
    "\n",
    "We'll create a pipeline to perform kmeans clustering. We will have to chose a value for k.\n",
    "\n",
    "#### ❓ What does the Kmeans algorithm do? What does this k represent? What would be a good value for it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly chose 3 for our number of clusters. Later on, we'll play with this number. We will then fit the pipeline to our model and get our cluster labels as a result. These labels will represent the cluster our observation belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Look up the documentation of the KMeans algortihm from sklearn.cluster. What type of linkage is used here? What does 'linkage criterion' mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('kmeans', KMeans(n_clusters=3, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(hotel_df)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = pipeline.named_steps['kmeans'].labels_\n",
    "print(\"Cluster Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Great, let's visualize this result! Try plotting your data and adding a color to it based on the labels. (Hint: you are supposed to run into a problem here. Explain why!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Principal Component Analysis (PCA) as a dimensionality reduction technique first \n",
    "\n",
    "In order to be able to plot our data, we will add a dimensionality reduction technique, called PCA. As we have already seen in our datasets, there are a lot of columns. If we transform the categorical variables into numerical ones using one-hot encoding, we end up with even more columns. More columns means a higher dimensionality, which means that our models grow more complex. \n",
    "\n",
    "More complex models means more potential for things going wrong. So, as usual, we will try to keep the model simple and the number of features compact. This is where the PCA comes in handy. \n",
    "\n",
    "As an example: consider the dataset shown on the picture below. \n",
    "\n",
    "In the original dataset, on the left, all the datapoins are represented with 3 variables. But, if you look closer at the data, you see that you do not need 3 variables, because it is possible to find a 2D plane that is actually able to represent most of our data. This plane is two-dimensional, yet it is able to represent most of our data. As you can see, not all the points lie exactly on the plane, but most of them are well represented by using only two dimensions. \n",
    "\n",
    "Usually, the extra benefit of exactly representing all points (using the full dimensionality) does not weigh up against approximately representing the data with less dimensions, and thereby creating a smaller, simpler version of the data. \n",
    "\n",
    "The PCA algorithm restructures the data into new dimensions and sorts them. The first dimension will be the most important one for explaining our data, the second one the second most important and so forth. Once we have applied the PCA algorithm, we can check how important these new dimensions actually are, using something called `explained variance ratio`, which will tell us how much of the total variance is explained by each of the dimensions.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\PCA.png\" style=\"background-color:white;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have updated the pipeline to transform the data into 2 PCA components. This way, we will be able to get a 2D view of our data with it's cluster labels. The PCA components are stored in a variable 'X_pca', with the first column being the first component and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('pca', PCA()), \n",
    "    ('kmeans', KMeans(n_clusters=3, random_state=4)),\n",
    "])\n",
    "\n",
    "pipeline.fit(hotel_df)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = pipeline.named_steps['kmeans'].labels_\n",
    "print(\"Cluster Labels:\", labels)\n",
    "\n",
    "# Get the PCA components (transformed features)\n",
    "X_pca = pipeline.named_steps['pca'].transform(pipeline.named_steps['preprocess'].transform(hotel_df))\n",
    "\n",
    "\n",
    "# Get explained variance ratio, which indicates the amount of variance captured by each PCA component.\n",
    "# This helps to understand how much information is retained in the reduced dimensions.\n",
    "# The explained variance ratio always sums up to 1. \n",
    "variance_per_dimension = pipeline.named_steps['pca'].explained_variance_ratio_\n",
    "\n",
    "# we will make a cumulative sum plot to see how many dimensions we need to capture most of the variance\n",
    "plt.plot(np.cumsum(variance_per_dimension))\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.grid()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows us how important each component is for explained the total data. The first three dimensions together represent about 30% of the total variance. This is not a lot, as there is still 70% of the variance which is not being accounted for. \n",
    "However, given that we can only plot three dimensions, for this particular dataset, this is the best we can do. In order to represent at least 80% of the total variance of the data, we need at least 13 dimensions. You can see the diminishing returns in the plot above: the more dimensions you add, the less extra information they will add. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Create a scatterplot of the PCA components colored by the cluster labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: `X_pca` contains our data, after PCA. `X_pca[:,0]` gives the data of the first dimension,  `X_pca[:,1]` of the second, and so forth. \n",
    "`labels` are the labels we just created using K-means. \n",
    "\n",
    "TIP: Normally, you always use the first dimensions, as these are the most important. If you want to see the difference between the importance of the first versus the other dimensions, try to plot two different random dimensions and observe if you still see clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ EXTRA: repeat this exercise in 3D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the optimal value of k \n",
    "Like we said before, the value of k is chosen randomly. We often iterate over different values of k, fitting models for each of the values and comparing the outcome. \n",
    "\n",
    "Since clustering is an unsupervised method, we don't have a definite way to show which clustering is better than the other. A visualisation that is commonly used is an inertia plot. Inertia represents the total distances from each plots to each cluster's center. The lower the inertia, the tighter the clusters. \n",
    "\n",
    "#### ❓ In theory, we could just take the value of k for which the inertia is lowest. This would mean the cluster are as tight as possible. Why is this a bad idea in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we are going to look for the 'elbow' in the plot. The elbow point is where adding more clusters starts giving diminishing returns, meaning the inertia decreases more slowly after this point. In the beginning, adding a cluster reduces the inertia by a significant amount. After a while, the curve starts to flatten, implying that splitting the data even more does not lead to significantly tighter clusters.\n",
    "\n",
    "We'll create an inertia plot for k between 2 and 10.\n",
    "\n",
    "#### ❓ Why don't we start with k = 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kmeans(X, k_range=range(2, 11)):\n",
    "    inertia = []\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=4)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        inertia.append(kmeans.inertia_)\n",
    "        \n",
    "       \n",
    "    \n",
    "    # Plot the results\n",
    "    fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Number of clusters (k)')\n",
    "    ax1.set_ylabel('Inertia', color=color)\n",
    "    ax1.plot(k_range, inertia, 'o-', color=color, label='Inertia')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True)\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title(\"KMeans Performance vs k\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data with your preprocessor\n",
    "X_transformed = pipeline.named_steps['preprocess'].transform(hotel_df)\n",
    "\n",
    "# Evaluate k in the range 2–10\n",
    "evaluate_kmeans(X_transformed, k_range=range(2, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Based on this plot, how many clusters would you choose? Create a plot (2D or 3D that visualizes these clusters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓EXTRA: repeat this clustering technique with another linkage criterion. Do you get different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Another clustering techique is hierarchical clustering. Write down how that works in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, hierarchical clustering on big datasets requires a lot of computational resources, because the distances between every datapoint is calculated after every split/combine. \n",
    "\n",
    "It's therefore not feasible to use this type of clustering on our hotel dataset. This is why we have added a little toy dataset about different animals that can be clustered into similar animals. \n",
    "\n",
    "This dataset contains 101 animals from a zoo, which are described using 16 different traits. All of the traits are already converted to numerical variables.\n",
    "There are 7 different classes of animal added to this dataset: Mammal, Bird, Reptile, Fish, Amphibian, Bug and Invertebrate. The class of the animal is left out of the dataset so we can see what our model does to the unlabeled data. The names of the animal (e.g. penguin, octopus...)  are recorded as well in the column 'names'. We will add them as an index to our dataset, so the nodes of the dendrogram (which we will talk about in a second) are labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoo_df = pd.read_csv(\"data/zoo.tab\", sep=\"\\t\")\n",
    "zoo_df.index = zoo_df['name']\n",
    "zoo_df = zoo_df.drop(columns=['name', 'type'])\n",
    "zoo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in hierarchical clustering is typically to create a dendrogram. A dendrogram visualizes the pairwise distances between observations using a tree-like structure. It shows how clusters are merged step by step, and it helps you decide how many clusters to choose in the final solution. Since the data is all numeric, the only transformation that's required now is to scale it. \n",
    "\n",
    "#### ❓Which variable would have a much larger impact than others if we wouldn't scale the data? Also look at the animals at the nodes. Are there any that are clustered completely wrong? If so, do you see any reason for that? What's drawing them close to the others?\n",
    "\n",
    "We'll first create a dendogram without scaling the data. Afterwards, we'll create a new dendrogram after scaling and compare both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Perform hierarchical clustering without scaling\n",
    "linked = linkage(zoo_df, method='ward')\n",
    "\n",
    "# Plot dendrogram using the index labels\n",
    "fig = ff.create_dendrogram(zoo_df, orientation='bottom', labels=zoo_df.index.astype(str).tolist())\n",
    "fig.update_layout(width=1200, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "zoo_scaled = scaler.fit_transform(zoo_df)\n",
    "\n",
    "# Perform hierarchical clustering with scaling\n",
    "linked = linkage(zoo_scaled, method='ward')\n",
    "\n",
    "# Plot dendrogram using the index labels\n",
    "fig = ff.create_dendrogram(zoo_scaled, orientation='bottom', labels=zoo_df.index.astype(str).tolist())\n",
    "fig.update_layout(width=1200, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Take a look at the dendogram above. To choose the number of clusters, draw a straight line through the dendrogram and see how many separate clusters you get at this point. For example, if you would draw the line at 9, you would have 2 separate clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, fill in the number of clusters you have decided on above in the code below. We'll cluster the data into these k groups and visualize again using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "# Step 2: Apply PCA to reduce dimensionality\n",
    "pca = PCA()\n",
    "zoo_pca = pca.fit_transform(zoo_scaled)\n",
    "\n",
    "# Step 3: Perform Agglomerative Clustering\n",
    "agg = AgglomerativeClustering(n_clusters=#YOURKHERE, linkage='ward')\n",
    "clusters = agg.fit_predict(zoo_pca)\n",
    "\n",
    "# Add cluster labels to the original DataFrame\n",
    "zoo_df['Cluster'] = clusters\n",
    "\n",
    "model = AgglomerativeClustering(\n",
    "    n_clusters= 8,      # adjust based on dendrogram\n",
    "    linkage='ward'\n",
    ")\n",
    "\n",
    "zoo_labels = model.fit_predict(zoo_df)\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "sc = ax.scatter(\n",
    "    zoo_pca[:, 0],\n",
    "    zoo_pca[:, 1],\n",
    "    zoo_pca[:, 2],\n",
    "    c=zoo_labels,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "ax.set_title(\"3D PCA Cluster Visualization\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analytics-machine-learning-book-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
